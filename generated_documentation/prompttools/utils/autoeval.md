```markdown
# autoeval.py Module

## Overview

The `autoeval.py` module is part of the `prompttools` package and provides functionality for automatically evaluating the quality of responses generated by a language model, such as GPT-4, based on a given prompt. It determines if the response is following the directions provided in the prompt.

## Functions

### `_get_messages`

```python
def _get_messages(prompt: str, response: str):
```

#### Description

This private function generates a list of messages formatted for evaluation by the OpenAI chat model. It uses the Jinja2 templating engine to create a user message that combines the prompt and response.

#### Parameters

- `prompt (str)`: The input prompt.
- `response (str)`: The model response.

#### Returns

- `List[Dict[str, str]]`: A list of dictionaries with two key-value pairs, where the keys are `role` and `content`. The `role` can be either `"system"` or `"user"`, and `content` contains the respective messages.

### `compute`

```python
def compute(prompt: str, response: str, model: str = "gpt-4") -> float:
```

#### Description

This function uses an OpenAI chat model, such as GPT-4, to automatically evaluate a given prompt/response pair. It returns a binary score indicating whether the response is correct (1.0) or incorrect (0.0) based on the model's judgment.

#### Parameters

- `prompt (str)`: The input prompt.
- `response (str)`: The model response.
- `model (str)`: The OpenAI chat model to use for generating an expected response. Defaults to GPT-4.

#### Returns

- `float`: A binary score, either 1.0 if the response is judged as "RIGHT" or 0.0 if judged as "WRONG".

#### Exceptions

- `PromptToolsUtilityError`: Raised if the `OPENAI_API_KEY` environment variable is not set.

### `evaluate`

```python
def evaluate(prompt: str, response: str, _metadata: Dict) -> float:
```

#### Description

This function is a wrapper around the `compute` function. It uses auto-evaluation to score the model response with "gpt-4" as the judge, returning a binary score.

#### Parameters

- `prompt (str)`: The input prompt.
- `response (str)`: The model response.
- `_metadata (Dict)`: A dictionary of metadata, currently not used in the function.

#### Returns

- `float`: A binary score, either 1.0 if the response is judged as "RIGHT" or 0.0 if judged as "WRONG".

### `autoeval_binary_scoring`

```python
def autoeval_binary_scoring(
    row: pandas.core.series.Series,
    prompt_column_name: str,
    response_column_name: str = "response",
) -> float:
```

#### Description

This function applies the auto-evaluation scoring to a row from a pandas DataFrame. It is designed to be used with the `apply` method of a DataFrame to score each row.

#### Parameters

- `row (pandas.core.series.Series)`: A row of data from the full DataFrame, which includes input, model response, and other metrics.
- `prompt_column_name (str)`: The name of the column that contains the input prompt.
- `response_column_name (str)`: The name of the column that contains the model's response. Defaults to `"response"`.

#### Returns

- `float`: A binary score, either 1.0 if the response is judged as "RIGHT" or 0.0 if judged as "WRONG".

## Constants

### `EVALUATION_SYSTEM_PROMPT`

A constant string that contains the system prompt used to instruct the OpenAI chat model on how to evaluate the responses.

### `EVALUATION_USER_TEMPLATE`

A constant string that serves as a Jinja2 template for formatting the user message that will be evaluated by the OpenAI chat model.

## Dependencies

- `os`: Standard library module to interact with the operating system.
- `typing.Dict`: Type hinting for dictionaries.
- `openai`: The OpenAI Python client library for accessing the OpenAI API.
- `pandas.core.series`: Data structure for a single column of a DataFrame.
- `jinja2`: Templating engine for Python.
- `error.PromptToolsUtilityError`: Custom exception class defined within the `prompttools` package.

## Usage

The module is used within a larger project to automatically score the quality of responses generated by a language model. It is typically used in scenarios where a large number of responses need to be evaluated quickly and consistently, such as in the development and testing of language models.

## License

The source code's license can be found in the `LICENSE` file in the root directory of this source tree.
```
